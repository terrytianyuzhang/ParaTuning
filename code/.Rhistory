interaction_order <- sieve.model$hyper.para.list$interaction_order
type <- sieve.model$type
M <- length(sieve.model$inf.list)
s.size.sofar <- sieve.model$s.size.sofar
if(s.size.sofar == 0){ #first time process the data
index_matrix <- sieve.model$index.matrix
index.row.prod <- sieve.model$index.row.prod
}else{
print('not first pass')
#####regenerate basis index matrix
max.basisN <- ceiling( max(J) * (s.size + s.size.sofar)^(1/(2*min(s) + 1))
) #the maximum number of basis functino needed to process all the data in X
xdim <- dim(X)[2] #dimension of predictors
index_matrix <- as.matrix(Sieve:::create_index_matrix(xdim, basisN = max.basisN, #the dimension of the index_matrix is specified by D (not xdim)
interaction_order = max(interaction_order))[1:max.basisN,],
ncol = xdim)
index.row.prod <- index_matrix[, 1]#index product will be used when determining the basis function-specific learning rate
index_matrix <- as.matrix(index_matrix[, -1], ncol = xdim)
sieve.model$index.matrix <- index_matrix
sieve.model$index.row.prod <- index.row.prod
}
for(i in 1:s.size){
i.sofar <- i + s.size.sofar #first pass, i.sofar = i
newx <- matrix(X[i,], nrow = 1)
newy <- Y[i]
max.J <- ceiling(max(J) * (i.sofar)^(1/(2*min(s) + 1))) #this is the maximum number of basis functions need to estimated at this step
Phi <- Sieve:::Design_M_C(newx, max.J, type, index_matrix) #one row design "matrix"
for(m in 1:M){
#####calculate rolling CV
###number of basis functions for this hyper-para combination
J.m <- J[sieve.model$inf.list[[m]]$hyper.para.index$J] * (i.sofar)^(1/(2* s[sieve.model$inf.list[[m]]$hyper.para.index$s]+ 1))
J.m <- ceiling(J.m)
beta.f <- sieve.model$inf.list[[m]]$beta.f
beta.f.int <- sieve.model$inf.list[[m]]$beta.f.int
if(length(beta.f) < J.m){ #check if more basis are used
beta.f <- c(beta.f, rep(0, J.m - length(beta.f)))
beta.f.int <- c(beta.f.int, rep(0, J.m - length(beta.f.int)))
}
fnewx <- crossprod(beta.f, Phi[1:J.m]) ###f_i(X_{i+1})
sieve.model$inf.list[[m]]$rolling.cv <- sieve.model$inf.list[[m]]$rolling.cv + (newy - fnewx)^2
##########update beta's
###overall learning rate
rn.m <- r0[sieve.model$inf.list[[m]]$hyper.para.index$r0] * (i.sofar)^(-1/(2* s[sieve.model$inf.list[[m]]$hyper.para.index$s]+ 1))
#####CORE UPDATING STEP
fnewx.int <- crossprod(beta.f.int, Phi[1:J.m])
res <- as.numeric(newy - fnewx.int)
beta.f.int <- beta.f.int + rn.m * res * (index.row.prod[1:J.m])^(-2*omega)*Phi[1:J.m]
beta.f <- (i.sofar-1)/i.sofar * beta.f + beta.f.int/i.sofar
################
sieve.model$inf.list[[m]]$beta.f <- beta.f
sieve.model$inf.list[[m]]$beta.f.int <- beta.f.int
}
}
#update the current number of sample processed
sieve.model$s.size.sofar <- s.size.sofar + s.size
######model comparison
rolling.cvs <- rep(1e10, M)
for(m in 1:M){
rolling.cvs[m] <- sieve.model$inf.list[[m]]$rolling.cv
}
print(rolling.cvs/sieve.model$s.size.sofar)
print(which.min(rolling.cvs))
return(sieve.model)
}
xdim <- 2
frho <- 'lineartensor'
frho <- 'additive'
#' @examples
xdim <- 2
frho <- 'additive'
type <- 'cosine'
TrainData <- Sieve:::GenSamples(s.size = 1e2, xdim = xdim,
frho.para = 1, frho = frho, noise.para = 0.1)
sieve.model <- sieve.sgd.preprocess(X = TrainData[,2:(xdim+1)],
type = type,
s = c(1,2),
r0 = c(0.5, 2, 4),
J = c(1, 4, 8))
#' @examples
xdim <- 2
frho <- 'additive'
type <- 'cosine'
TrainData <- Sieve:::GenSamples(s.size = 1e4, xdim = xdim,
frho.para = 1, frho = frho, noise.para = 0.1)
sieve.model <- sieve.sgd.preprocess(X = TrainData[,2:(xdim+1)],
type = type,
s = c(1,2),
r0 = c(0.5, 2, 4),
J = c(1, 4, 8))
sieve.model <- sieve.sgd.solver(sieve.model = sieve.model,
X = TrainData[,2:(xdim+1)],
Y  = TrainData[,1])
library(Sieve)
#generate training data
TrainData <- GenSamples(s.size = 1e4, xdim = xdim,
frho.para = frho.para,
frho = frho, noise.para = 0.1)
#' @examples
frho.para <- xdim <- 2 ##predictor dimension
frho <- 'additive' ###truth is a sum of absolute functions
type <- 'cosine' ###use cosine functions as the basis functions
#generate training data
TrainData <- GenSamples(s.size = 1e4, xdim = xdim,
frho.para = frho.para,
frho = frho, noise.para = 0.1)
frho.para <- xdim <- 2 ##predictor dimension
frho <- 'additive' ###truth is a sum of absolute functions
type <- 'cosine' ###use cosine functions as the basis functions
#generate training data
TrainData <- GenSamples(s.size = 1e4, xdim = xdim,
frho.para = frho.para,
frho = frho, noise.para = 0.1)
#train the model
sieve.model <- sieve.sgd.preprocess(X = TrainData[,2:(xdim+1)],
type = type,
s = c(1,2),
r0 = c(0.5, 2, 4),
J = c(1, 4, 8))
##sieve-SGD can do multiple passes over the data (epochs > 1).
##watch out overfitting!
sieve.model <- sieve.sgd.solver(sieve.model = sieve.model,
X = TrainData[,2:(xdim+1)],
Y  = TrainData[,1])
frho.para <- xdim <- 2 ##predictor dimension
frho <- 'additive' ###truth is a sum of absolute functions
type <- 'cosine' ###use cosine functions as the basis functions
#generate training data
TrainData <- GenSamples(s.size = 1e3, xdim = xdim,
frho.para = frho.para,
frho = frho, noise.para = 0.1)
#preprocess the model
sieve.model <- sieve.sgd.preprocess(X = TrainData[,2:(xdim+1)],
type = type,
s = c(1,2),
r0 = c(0.5, 2, 4),
J = c(1, 4, 8))
##train the model
sieve.model <- sieve.sgd.solver(sieve.model = sieve.model,
X = TrainData[,2:(xdim+1)],
Y  = TrainData[,1])
##sieve-SGD can do multiple passes over the data, just like other SGD methods.
##usually a second pass can still improve the prediction accuracy
##watch out overfitting when performing multiple passes!
sieve.model <- sieve.sgd.solver(sieve.model = sieve.model,
X = TrainData[,2:(xdim+1)],
Y  = TrainData[,1])
frho.para <- xdim <- 2 ##predictor dimension
frho <- 'additive' ###truth is a sum of absolute functions
type <- 'cosine' ###use cosine functions as the basis functions
#generate training data
TrainData <- GenSamples(s.size = 1e3, xdim = xdim,
frho.para = frho.para,
frho = frho, noise.para = 0.1)
#preprocess the model
sieve.model <- sieve.sgd.preprocess(X = TrainData[,2:(xdim+1)],
type = type,
s = c(1,2),
r0 = c(0.5, 2, 4),
J = c(1, 4, 8))
##train the model
sieve.model <- sieve.sgd.solver(sieve.model = sieve.model,
X = TrainData[,2:(xdim+1)],
Y  = TrainData[,1])
##sieve-SGD can do multiple passes over the data, just like other SGD methods.
##usually a second pass can still improve the prediction accuracy
##watch out overfitting when performing multiple passes!
sieve.model <- sieve.sgd.solver(sieve.model = sieve.model,
X = TrainData[,2:(xdim+1)],
Y  = TrainData[,1])
a <- vector("list", 2)
a
risk.score.list <- vector("list",2)
risk.score.list
####locally verify if the covariance penalization works
####i will normalize everything just as in lassosum
set.sed(2019)
#####generate the original training data #####
library(MASS)
# set.seed(2019)
n <- 100
s <- 5
x <- mvrnorm(n, mu = rep(0, s), Sigma = diag(1,s))
beta <- c(1, 0.5, 0.1, -0.5, -0.1)
risk.score <- x %*% beta
risk.score <- exp(risk.score)/(1 + exp(risk.score))
risk.score <- (risk.score - min(risk.score))/ (max(risk.score) - min(risk.score))
###generate y
y <- rbinom(n, size = 1, prob = risk.score)
summary(y)
###attach useless x
p <- 700
x <- cbind(x, mvrnorm(n, mu = rep(0, p - s), Sigma = diag(1,p - s)))
#############################################
#####generate the testing data #####
testn <- 5e3
testx <- mvrnorm(testn, mu = rep(0, s), Sigma = diag(1,s))
risk.score.test <- testx %*% beta
risk.score.test <- exp(risk.score.test)/(1 + exp(risk.score.test))
risk.score.test <- (risk.score.test - min(risk.score.test))/ (max(risk.score.test) - min(risk.score.test))
###generate y
testy <- rbinom(testn, size = 1, prob = risk.score.test)
summary(testy)
###attach useless x
testx <- cbind(testx, mvrnorm(testn, mu = rep(0, p - s), Sigma = diag(1,p - s)))
#############################################
#######train the model with glmnet###########
library(glmnet)
normalize.mine <- function(x){
return((x - mean(x))/(sqrt(sum((x - mean(x))^2))))
}
####normalize x, y###
y <- normalize.mine(y)
for(i in 1:p){
x[,i] <- normalize.mine(x[,i])
}
glmnet.fit <- glmnet(x = x, y = y)
summary(glmnet.fit)
dim(glmnet.fit$beta)
head(glmnet.fit$beta)
##beta0 corresponds to a smallish lambda
beta0 <- glmnet.fit$beta[, ceiling(NCOL(glmnet.fit$beta)/2)]
lambda.org <- glmnet.fit$lambda
###for each beta_lambda, calculate training error
trn.est <- rep(0, length(lambda.org))
for(i in 1:length(lambda.org)){
temp.mse <- mean((y - x %*% glmnet.fit$beta[,i])^2)
trn.est[i] <- temp.mse
}
#############################################
#######generate boostrap data################
###size of reference panel
refn <- n
newx <- mvrnorm(refn, mu = rep(0, p), Sigma = diag(1,p))
for(i in 1:p){
newx[,i] <- normalize.mine(newx[,i])
}
bootn <- refn
##there are repeated covariates
if(refn < bootn){
newx <- newx[sample(1:refn, size = bootn, replace = T),]
}else{
newx <- newx[sample(1:refn, size = bootn, replace = F),]
}
risk.score.new <- newx %*% beta0
risk.score.new <- (risk.score.new - min(risk.score.new))/ (max(risk.score.new) - min(risk.score.new))
####
B <- 5
boot.Y <- matrix(0, nrow = bootn, ncol = B)
for(b in 1:B){
###generate y
newy <- rbinom(bootn, size = 1, prob = risk.score.new)
newy <- normalize.mine(newy)
boot.Y[, b] <- newy
}
cov.est <- rep(0, length(lambda.org))
for(i in 1:length(lambda.org)){
temp.cov <- 0
for(b in 1:B){
#####use the original lambda
# glmnet.boot <- glmnet(x = newx, y = boot.Y[,b],
#                       lambda = lambda.org[i])
####use the square-root rule
glmnet.boot <- glmnet(x = newx, y = boot.Y[,b],
lambda = lambda.org[i] * sqrt(n/bootn))
beta.boot <- glmnet.boot$beta
risk.score.boot <- as.numeric(newx %*% beta.boot)
temp.cov <- temp.cov + cov(risk.score.boot, boot.Y[,b])
}
temp.cov <- temp.cov/B
cov.est[i] <- temp.cov
}
#############################################
#####use AUC to determine which is the best lambda#####
glmnet.cvfit <- cv.glmnet(x = x, y = y,
lambda = lambda.org)
print(which.min(glmnet.cvfit$cvm))
cv.index <- which.min(glmnet.cvfit$cvm)
# glmnet.cvfit <- cv.glmnet(x = x, y = y,
#                           lambda = lambda.org,
#                           type.measure = "auc",
#                           family  = "binomial")
# print(which.max(glmnet.cvfit$cvm))
# glmnet.cvfit$cvm
#############################################
###########combine training error and covaraince est##########
cov.est.norm <- cov.est/cov.est[length(lambda.org)] * trn.est[length(lambda.org)]
print(which.min(2*cov.est + trn.est))
cov.index <- which.min(2*cov.est + trn.est)
#############################################
#############compare the difference###################
library(pROC)
cv.test.score <- testx %*% glmnet.fit$beta[,cv.index]
cov.test.score <- testx %*% glmnet.fit$beta[,cov.index]
auc(testy, as.numeric(cv.test.score))
auc(testy, as.numeric(cov.test.score))
plot(cov.est)
plot(trn.est)
plot(2*cov.est + trn.est)
#############################################
#####generate the original training data #####
library(MASS)
# set.seed(2019)
n <- 100
s <- 5
x <- mvrnorm(n, mu = rep(0, s), Sigma = diag(1,s))
beta <- c(1, 0.5, 0.1, -0.5, -0.1)
risk.score <- x %*% beta
risk.score <- exp(risk.score)/(1 + exp(risk.score))
risk.score <- (risk.score - min(risk.score))/ (max(risk.score) - min(risk.score))
###generate y
y <- rbinom(n, size = 1, prob = risk.score)
summary(y)
###attach useless x
p <- 700
x <- cbind(x, mvrnorm(n, mu = rep(0, p - s), Sigma = diag(1,p - s)))
#############################################
#####generate the testing data #####
testn <- 5e3
testx <- mvrnorm(testn, mu = rep(0, s), Sigma = diag(1,s))
risk.score.test <- testx %*% beta
risk.score.test <- exp(risk.score.test)/(1 + exp(risk.score.test))
risk.score.test <- (risk.score.test - min(risk.score.test))/ (max(risk.score.test) - min(risk.score.test))
###generate y
testy <- rbinom(testn, size = 1, prob = risk.score.test)
summary(testy)
###attach useless x
testx <- cbind(testx, mvrnorm(testn, mu = rep(0, p - s), Sigma = diag(1,p - s)))
#############################################
#######train the model with glmnet###########
library(glmnet)
normalize.mine <- function(x){
return((x - mean(x))/(sqrt(sum((x - mean(x))^2))))
}
####normalize x, y###
y <- normalize.mine(y)
for(i in 1:p){
x[,i] <- normalize.mine(x[,i])
}
glmnet.fit <- glmnet(x = x, y = y)
summary(glmnet.fit)
dim(glmnet.fit$beta)
head(glmnet.fit$beta)
##beta0 corresponds to a smallish lambda
beta0 <- glmnet.fit$beta[, ceiling(NCOL(glmnet.fit$beta)/2)]
lambda.org <- glmnet.fit$lambda
###for each beta_lambda, calculate training error
trn.est <- rep(0, length(lambda.org))
for(i in 1:length(lambda.org)){
temp.mse <- mean((y - x %*% glmnet.fit$beta[,i])^2)
trn.est[i] <- temp.mse
}
#############################################
#######generate boostrap data################
###size of reference panel
refn <- n
newx <- mvrnorm(refn, mu = rep(0, p), Sigma = diag(1,p))
for(i in 1:p){
newx[,i] <- normalize.mine(newx[,i])
}
bootn <- refn
##there are repeated covariates
if(refn < bootn){
newx <- newx[sample(1:refn, size = bootn, replace = T),]
}else{
newx <- newx[sample(1:refn, size = bootn, replace = F),]
}
risk.score.new <- newx %*% beta0
risk.score.new <- (risk.score.new - min(risk.score.new))/ (max(risk.score.new) - min(risk.score.new))
####
B <- 1
boot.Y <- matrix(0, nrow = bootn, ncol = B)
for(b in 1:B){
###generate y
newy <- rbinom(bootn, size = 1, prob = risk.score.new)
newy <- normalize.mine(newy)
boot.Y[, b] <- newy
}
cov.est <- rep(0, length(lambda.org))
for(i in 1:length(lambda.org)){
temp.cov <- 0
for(b in 1:B){
#####use the original lambda
# glmnet.boot <- glmnet(x = newx, y = boot.Y[,b],
#                       lambda = lambda.org[i])
####use the square-root rule
glmnet.boot <- glmnet(x = newx, y = boot.Y[,b],
lambda = lambda.org[i] * sqrt(n/bootn))
beta.boot <- glmnet.boot$beta
risk.score.boot <- as.numeric(newx %*% beta.boot)
temp.cov <- temp.cov + cov(risk.score.boot, boot.Y[,b])
}
temp.cov <- temp.cov/B
cov.est[i] <- temp.cov
}
#############################################
#####use AUC to determine which is the best lambda#####
glmnet.cvfit <- cv.glmnet(x = x, y = y,
lambda = lambda.org)
print(which.min(glmnet.cvfit$cvm))
cv.index <- which.min(glmnet.cvfit$cvm)
# glmnet.cvfit <- cv.glmnet(x = x, y = y,
#                           lambda = lambda.org,
#                           type.measure = "auc",
#                           family  = "binomial")
# print(which.max(glmnet.cvfit$cvm))
# glmnet.cvfit$cvm
#############################################
###########combine training error and covaraince est##########
cov.est.norm <- cov.est/cov.est[length(lambda.org)] * trn.est[length(lambda.org)]
print(which.min(2*cov.est + trn.est))
cov.index <- which.min(2*cov.est + trn.est)
#############################################
#############compare the difference###################
library(pROC)
cv.test.score <- testx %*% glmnet.fit$beta[,cv.index]
cov.test.score <- testx %*% glmnet.fit$beta[,cov.index]
auc(testy, as.numeric(cv.test.score))
auc(testy, as.numeric(cov.test.score))
plot(cov.est)
plot(trn.est)
plot(2*cov.est + trn.est)
#############################################
####
beta.boot
glmnet.boot$beta[,99]
glmnet.boot$beta
cov.est
i <- length(lambda.org)
temp.cov <- 0
for(b in 1:B){
#####use the original lambda
# glmnet.boot <- glmnet(x = newx, y = boot.Y[,b],
#                       lambda = lambda.org[i])
####use the square-root rule
glmnet.boot <- glmnet(x = newx, y = boot.Y[,b],
lambda = lambda.org[i] * sqrt(n/bootn))
beta.boot <- glmnet.boot$beta
risk.score.boot <- as.numeric(newx %*% beta.boot)
temp.cov <- temp.cov + cov(risk.score.boot, boot.Y[,b])
}
temp.cov <- temp.cov/B
temp.cov
cov(risk.score.boot, boot.Y[,b])
cov.est <- rep(0, length(lambda.org))
i <- length(lambda.org)
temp.cov <- 0
for(b in 1:B){
#####use the original lambda
# glmnet.boot <- glmnet(x = newx, y = boot.Y[,b],
#                       lambda = lambda.org[i])
####use the square-root rule
glmnet.boot <- glmnet(x = newx, y = boot.Y[,b],
lambda = lambda.org[i] * sqrt(n/bootn))
beta.boot <- glmnet.boot$beta
risk.score.boot <- as.numeric(newx %*% beta.boot)
temp.cov <- temp.cov + cov(risk.score.boot, boot.Y[,b])
}
temp.cov
temp.cov <- temp.cov/B
cov.est[i] <- temp.cov
cov.est
cov.est <- rep(0, length(lambda.org))
for(i in 1:length(lambda.org)){
temp.cov <- 0
for(b in 1:B){
#####use the original lambda
# glmnet.boot <- glmnet(x = newx, y = boot.Y[,b],
#                       lambda = lambda.org[i])
####use the square-root rule
glmnet.boot <- glmnet(x = newx, y = boot.Y[,b],
lambda = lambda.org[i] * sqrt(n/bootn))
beta.boot <- glmnet.boot$beta
risk.score.boot <- as.numeric(newx %*% beta.boot)
temp.cov <- temp.cov + cov(risk.score.boot, boot.Y[,b])
}
temp.cov <- temp.cov/B
cov.est[i] <- temp.cov
}
cov.est
lambda.org[1]
lambda.org[5]
i <- 1
temp.cov <- 0
for(b in 1:B){
#####use the original lambda
# glmnet.boot <- glmnet(x = newx, y = boot.Y[,b],
#                       lambda = lambda.org[i])
####use the square-root rule
glmnet.boot <- glmnet(x = newx, y = boot.Y[,b],
lambda = lambda.org[i] * sqrt(n/bootn))
beta.boot <- glmnet.boot$beta
risk.score.boot <- as.numeric(newx %*% beta.boot)
temp.cov <- temp.cov + cov(risk.score.boot, boot.Y[,b])
}
temp.cov
beta.boot
sum(beta.boot)
nfold <- 10
##########split training and testing########
val.index <- sample(1:s.size, floor(s.size/nfold))
val.index
s.size
train.index <- 1:s.size[-val.index]
train.index
-val.index
train.index <- (1:s.size)[-val.index]
train.index
system.time(gnt<-read.plink(bed=paste0("/raid6/Tianyu/PRS/bert_sample/",anc,".TUNE/CHR/",anc,".TUNE-chr",chr,".bed"),
bim=paste0("/raid6/Tianyu/PRS/bert_sample/",anc,".TUNE/CHR/",anc,".TUNE-chr",chr,".bim"),
fam=paste0("/raid6/Tianyu/PRS/bert_sample/",anc,".TUNE/CHR/",anc,".TUNE-chr",chr,"_boost_val.fam"))
)
val.index <- sample(1:s.size, floor(s.size/nfold))
